{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Esercitazione 2 - Regressione Lineare**","metadata":{}},{"cell_type":"markdown","source":"## Boston Housing dataset","metadata":{}},{"cell_type":"markdown","source":"Questo dataset contiene informazioni raccolte dal U.S. Census Service riguardanti le abitazioni nell'area di Boston, Massachusetts. È stato ottenuto dall'archivio StatLib (http://lib.stat.cmu.edu/datasets/boston) ed è stato ampiamente utilizzato in letteratura per fare benchmark di algoritmi. \n\nIl dataset contiene informazioni su 506 case, divise in 14 variabili.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:30.824689Z","iopub.execute_input":"2025-04-03T19:05:30.825068Z","iopub.status.idle":"2025-04-03T19:05:30.829763Z","shell.execute_reply.started":"2025-04-03T19:05:30.825036Z","shell.execute_reply":"2025-04-03T19:05:30.828469Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import pandas as pd \nfrom sklearn.utils import shuffle\nfrom pandas import read_csv\n\nfrom sklearn.datasets import fetch_openml\nimport pandas as pd\n\n# Scarica il Boston Housing Dataset da OpenML\nboston = fetch_openml(name=\"Boston\", version=1, as_frame=True)\n\n# Estrai i dati (features) e il target (valore mediano delle abitazioni)\nX = boston.data\ny = boston.target\n\nX, y = shuffle(X, y, random_state=0)\nprint(f\"Features shape: {X.shape}, targets shape:  {y.shape}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:30.836314Z","iopub.execute_input":"2025-04-03T19:05:30.836675Z","iopub.status.idle":"2025-04-03T19:05:30.877816Z","shell.execute_reply.started":"2025-04-03T19:05:30.836647Z","shell.execute_reply":"2025-04-03T19:05:30.876747Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Features shape: (506, 13), targets shape:  (506,)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## `np.c_` in NumPy\n\nL'oggetto `np.c_` in NumPy è una **scorciatoia** per concatenare array lungo il secondo asse (cioè, le colonne).\n\n## Utilizzo\n```python\nnp.c_[array1, array2, ...]\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Generate two random 2x3 matrices\nmatrice1 = np.random.rand(2, 3)\nmatrice2 = np.random.rand(2, 3)\n\n# Concatenate the matrices along columns\nrisultato = np.c_[matrice1, matrice2]\n\nprint(\"Matrice 1:\",matrice1.shape)\n\nprint(\"\\nMatrice 2:\",matrice2.shape)\n\nprint(\"\\nMatrice concatenata:\",risultato.shape)","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:30.879112Z","iopub.execute_input":"2025-04-03T19:05:30.879436Z","iopub.status.idle":"2025-04-03T19:05:30.887340Z","shell.execute_reply.started":"2025-04-03T19:05:30.879373Z","shell.execute_reply":"2025-04-03T19:05:30.886021Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Matrice 1: (2, 3)\n\nMatrice 2: (2, 3)\n\nMatrice concatenata: (2, 6)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"**Divisione del dataset**\n\nIl primo passaggio è quello di dividere i dati in train set, validation set e test set. Utilizza il 60% dei dati per il training set, il 20% per il validation e il restante 20% per il test set. Considerato che il nostro dataset possiede 506 osservazioni mi aspetto che:\n\n- Il **training set** avrà 303 osservazioni.\n- Il **validation set** avrà 101 osservazioni.\n- Il **test set** avrà 101 osservazioni.\n\nIn reatà il test set avrà 102 osservazioni per via delle approssimazioni.\n\n","metadata":{}},{"cell_type":"code","source":"# Divisione del dataset\n\ntrain_porzione = 0.6  \nval_porzione = 0.2  \ntest_porzione = 0.2\n\ntrain_dim= int(X.shape[0]*train_porzione)\nval_dim= int(X.shape[0]*val_porzione)\nX_train=X[:train_dim]\nY_train=y[:train_dim]\nX_valadation=X[train_dim:train_dim+val_dim]\nY_valadation=y[train_dim:train_dim+val_dim]\n\nX_test=X[train_dim+val_dim:]\nY_test=y[train_dim+val_dim:]\n\nprint(\"train shape: \", X_train.shape)\nprint(\"test shape: \", X_valadation.shape)\nprint(\"valadation shape: \", X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:30.889689Z","iopub.execute_input":"2025-04-03T19:05:30.890006Z","iopub.status.idle":"2025-04-03T19:05:30.914196Z","shell.execute_reply.started":"2025-04-03T19:05:30.889978Z","shell.execute_reply":"2025-04-03T19:05:30.913140Z"},"trusted":true},"outputs":[{"name":"stdout","text":"train shape:  (303, 13)\ntest shape:  (101, 13)\nvaladation shape:  (102, 13)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### **Esercizio 1: Costruisci una Pipeline di Regressione Lineare Standardizzata**\n\n**Step 1:** Standardizza i dataset di addestramento, validazione e test. Usa `StandardScaler` di scikit-learn.  \n\n**Step 2:** Aggiungi una feature costante (bias) ai dati concatenando una colonna di uno ad ogni dataset.  \n\n**Step 3:** Implementa la soluzione in forma chiusa per l'addestramento di un modello di regressione lineare. \n \n**Step 4:** Valuta il modello calcolando il Mean Absolute Error (MAE) sui dataset di addestramento, validazione e test.\n","metadata":{}},{"cell_type":"markdown","source":"### **Guida**\n\n1. **StandardScaler**:\n   - Utilizza `StandardScaler` da `sklearn.preprocessing` per standardizzare i dati.\n   - Il metodo `fit_transform` calcola la media e la varianza dei dati di addestramento e li scala di conseguenza.\n   - Utilizza `transform` per standardizzare i dati di validazione e test utilizzando gli stessi parametri. Utilizziamo il metodo `transform` perchè non calcola i parametri di scaling (media e std). In questo modo ci assicuriamo che i dati di training e quelli di validation e test vengano scalati in modo uguale. Se usassimo `fit_transform` avremmo degli scaling diversi.\n\n2. **Aggiunta di una Caratteristica Costante**:\n   - Utilizza `np.c_` per concatenare una colonna di uno alle matrici delle caratteristiche. Questo è importante per includere il termine di intercetta nella regressione lineare.\n\n3. **Soluzione in Forma Chiusa per la Regressione Lineare**:\n   - La soluzione in forma chiusa è:\n\n     $$\\theta = (X^T X)^{-1} X^T y$$\n\n   - Per calcolare la trasposta di una matrice possiamo utilizzare l' attributo `.T` di cui ogni array è dotato.\n\n   - Utilizza `np.linalg.inv` di NumPy per l'inversione della matrice e l'operatore `@` per la moltiplicazione matriciale.\n  \n   - Puoi utilizzare l'operatore @ per eseguire l'operazione np.dot (`A @ B` è equivalente a `np.dot(A, B)`).\n\n4. **Mean Absolute Error (MAE)**:\n   - L'MAE si calcola come:\n\n     $$\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$$\n\n   - Utilizza `np.mean` e `np.abs` per calcolarlo.\n","metadata":{}},{"cell_type":"code","source":"# Step 1 - Normalizzazione dei dati. Dobbiamo normalizzare le features \n# sia del training set, validation set e test set.\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Utilizziamo il metodo .fit_transform() dello scaler per normalizzare le feature di training.\n\n# Per normalizzare le feature di validation e test utilizziamo il metodo .transform()\nscaler = StandardScaler()\n\nx_train_norm=scaler.fit_transform(X_train)\nx_val_norm=scaler.transform(X_valadation)\nx_test_norm=scaler.transform(X_test)\n\nprint(x_train_norm)\nprint(x_val_norm)\nprint(x_test_norm)\n# svolgimento...","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:30.915966Z","iopub.execute_input":"2025-04-03T19:05:30.916472Z","iopub.status.idle":"2025-04-03T19:05:30.963519Z","shell.execute_reply.started":"2025-04-03T19:05:30.916434Z","shell.execute_reply":"2025-04-03T19:05:30.962118Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[[-0.44804325 -0.47722561 -1.20888589 ... -0.72714928  0.26235409\n  -0.7767965 ]\n [ 0.77103386 -0.47722561  1.02487948 ...  0.80741501  0.17182767\n  -0.46725622]\n [-0.44179028 -0.47722561  0.3920294  ... -0.95965902  0.4475036\n  -0.33015391]\n ...\n [-0.44947979  1.23173418 -0.73236932 ... -0.40163564  0.47907794\n  -1.31531178]\n [ 0.05336461 -0.47722561  1.02487948 ...  0.80741501  0.01695355\n   0.19140026]\n [-0.44583123 -0.47722561  0.23268947 ...  0.10988579  0.47068476\n  -0.52803354]]\n[[-0.41465286 -0.47722561 -0.20775013 ... -0.02962005  0.41712829\n   0.7765586 ]\n [-0.40351297 -0.47722561 -0.20775013 ... -0.02962005  0.46219166\n  -0.34994187]\n [-0.37322047 -0.47722561 -0.47231454 ...  1.1794306   0.47907794\n  -0.64676132]\n ...\n [-0.40213495 -0.47722561 -0.76393666 ... -0.49463953  0.23107951\n  -0.91531224]\n [-0.14693384 -0.47722561  1.24735409 ... -1.75019213  0.08030206\n   2.1885711 ]\n [ 0.08802262 -0.47722561  1.24735409 ... -1.75019213  0.47907794\n   1.92002018]]\n[[-0.45201503 -0.47722561 -0.91576055 ...  0.80741501  0.40413885\n  -0.86018863]\n [-0.44822415 -0.47722561 -1.32613602 ... -0.30863174  0.46568882\n  -0.74570113]\n [ 0.73786827 -0.47722561  1.02487948 ...  0.80741501  0.28753362\n   0.6733785 ]\n ...\n [-0.43690469 -0.47722561 -0.19121486 ... -0.30863174  0.45519735\n  -0.35842243]\n [-0.42649106 -0.47722561 -0.65720898 ... -0.26212979  0.43751172\n   0.84298962]\n [-0.43847957 -0.47722561 -1.08712613 ... -0.86665512  0.47907794\n   0.26207156]]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Step 2 - Aggiunta di una feature costante\n\n# creiamo un vettore di 1 da aggiungere come feature costante. \n# ATTENZIONE: questo vettore deve avere le stesse righe del set a cui viene aggiunto. \n# Uno uguale per tutti non va bene\n\ncolonne_train=np.ones(x_train_norm.shape[0])\ncolonne_val=np.ones(x_val_norm.shape[0])\ncolonne_test=np.ones(x_test_norm.shape[0])\n\ntrain_x=np.c_[colonne_train,x_train_norm]\nval_x=np.c_[colonne_val,x_val_norm]\ntest_x=np.c_[colonne_test,x_test_norm]\n\nprint(train_x.shape)\nprint(val_x.shape)\nprint(test_x.shape)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:30.964841Z","iopub.execute_input":"2025-04-03T19:05:30.965290Z","iopub.status.idle":"2025-04-03T19:05:30.979870Z","shell.execute_reply.started":"2025-04-03T19:05:30.965211Z","shell.execute_reply":"2025-04-03T19:05:30.978735Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(303, 14)\n(101, 14)\n(102, 14)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Step 3 - Applichiamo la formula matematica della regressione lineare\n\n# ATTENZIONE: stiamo per effettuare operazioni tra matrici e vettori, \n# non si tratta di una semplice formula matematica, stiamo attenti a quali operatori utilizzare e quanto\n#Per calcolare la trasposta di una matrice possiamo utilizzare l' attributo .T di cui ogni array è dotato.\n\n#Utilizza np.linalg.inv di NumPy per l'inversione della matrice e l'operatore @ per la moltiplicazione matriciale.\n\n#Puoi utilizzare l'operatore @ per eseguire l'operazione np.dot (A @ B è equivalente a np.dot(A, B)).\n\nx_train_trasposta=train_x.T\nk=np.linalg.inv(x_train_trasposta@train_x)@x_train_trasposta@Y_train\n\nprint(k)\ny_train_pred=train_x@k\ny_test_pred=test_x@k\ny_val_pred=val_x@k\nprint(y_train_pred.shape)\nprint(y_test_pred.shape)\nprint(y_val_pred.shape)","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:30.981168Z","iopub.execute_input":"2025-04-03T19:05:30.981530Z","iopub.status.idle":"2025-04-03T19:05:31.006330Z","shell.execute_reply.started":"2025-04-03T19:05:30.981503Z","shell.execute_reply":"2025-04-03T19:05:31.005033Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[22.16138614 -0.50926449  1.04570167  0.08640982  0.43014273 -1.8975044\n  2.62964478 -0.02436637 -2.95441032  2.92892785 -2.41571656 -1.74144698\n  1.12132994 -3.56543566]\n(303,)\n(102,)\n(101,)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Step 4 - Calcolo MAE\n\n# Calcoliamo l'errore medio assoluto (MAE) per il training set, validation set e test set.\n# Utlizziamo la formula specificata nella guida.\n#Utilizza np.mean e np.abs per calcolarlo.\n\n\nm_train=np.mean(np.abs(y_train_pred-Y_train))\nm_val=np.mean(np.abs(y_val_pred-Y_valadation))\nm_test=np.mean(np.abs(y_test_pred-Y_test))\n\nprint(m_train)\nprint(m_val)\nprint(m_test)","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:31.008729Z","iopub.execute_input":"2025-04-03T19:05:31.009097Z","iopub.status.idle":"2025-04-03T19:05:31.040131Z","shell.execute_reply.started":"2025-04-03T19:05:31.009067Z","shell.execute_reply":"2025-04-03T19:05:31.038968Z"},"trusted":true},"outputs":[{"name":"stdout","text":"3.2496837441036983\n3.5977784744283965\n3.0197366706166866\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"### **Esercizio: Costruisci una pipeline di Regressione Lineare Standardizzata utilizzando `scikit-learn`** \n\n**Step 1 & 2:** Step 1 e 2 sono uguali a quanto fatto prima.\n\n**Step 3:** Utilizza `LinearRegression()` di scikit-learn per addestrare un modello di regressione lineare.  \n\n**Step 4:** Valuta il modello calcolando il Mean Absolute Error (MAE) sui dataset di addestramento, validazione e test, utilizzando `mean_absolute_error()` da `sklearn.metrics`.\n","metadata":{}},{"cell_type":"markdown","source":"## `LinearRegression` da Scikit-Learn\n\nLa classe `LinearRegression` in Scikit-Learn viene utilizzata per eseguire la **regressione lineare**, adattando un modello lineare al dataset.\n\n## **Sintassi**\n```python\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\n# Dati di esempio\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.array([10, 15, 20, 25])\n\n# Adatta il modello ai dati\nmodel.fit(X, y)\n\n# Predici nuovi valori\nX_new = np.array([[3, 5], [5, 9]])\npredictions = model.predict(X_new)\n","metadata":{}},{"cell_type":"markdown","source":"## `mean_absolute_error` da Scikit-Learn\n\nLa funzione `mean_absolute_error` calcola l'**errore assoluto medio** (MAE) tra i valori target reali e quelli predetti.\n\n## **Sintassi**\n```python\nsklearn.metrics.mean_absolute_error(y_true, y_pred)\n","metadata":{}},{"cell_type":"markdown","source":"### **Guida**\n\n1. **Istanziare e allenare un modello di regressione lineare**:\n    \n    - Istanziamo una classe `LinearRegression` per creare il modello.\n    - Utilizziamo il metodo `.fit()` per allenare il modello con i dati di training.\n\n2. **Effettuare predizioni con il modello**:\n\n    - Utiliziamo il metodo `.predict()` del modello per effettuare le predizioni. Effettuiamo le predizioni per tutti i set che abbiamo (train, validation e test).\n\n3. **Calcolo della MAE**: \n\n    - Calcolare MAE su tutti i set utilizzando la funzione `mean_abslute_error`\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:31.041863Z","iopub.execute_input":"2025-04-03T19:05:31.042276Z","iopub.status.idle":"2025-04-03T19:05:31.061995Z","shell.execute_reply.started":"2025-04-03T19:05:31.042245Z","shell.execute_reply":"2025-04-03T19:05:31.060754Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Step 1 - Istanziare e allenare il modello di regressione lineare.\nregressione_lineare=LinearRegression()\n\nregressione_lineare.fit(train_x,Y_train)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:31.063085Z","iopub.execute_input":"2025-04-03T19:05:31.063603Z","iopub.status.idle":"2025-04-03T19:05:31.086593Z","shell.execute_reply.started":"2025-04-03T19:05:31.063540Z","shell.execute_reply":"2025-04-03T19:05:31.085483Z"},"trusted":true},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"LinearRegression()","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# Step 2 - Effettuare predizioni\ny_pred_train=regressione_lineare.predict(train_x)\ny_pred_val=regressione_lineare.predict(val_x)\ny_pred_test=regressione_lineare.predict(test_x)\n\nprint(y_pred_train.shape)\nprint(y_pred_val.shape)\nprint(y_pred_test.shape)","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:31.087412Z","iopub.execute_input":"2025-04-03T19:05:31.087699Z","iopub.status.idle":"2025-04-03T19:05:31.107674Z","shell.execute_reply.started":"2025-04-03T19:05:31.087675Z","shell.execute_reply":"2025-04-03T19:05:31.106614Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(303,)\n(101,)\n(102,)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Step 3 - Calcolo MAE\n\nmae_train=mean_abslute_error (y_train,y_pred_train)\nmae_val=mean_abslute_error (y_valadation,y_pred_val)\nmae_test=mean_abslute_error(y_test,y_pred_test)","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:31.108599Z","iopub.execute_input":"2025-04-03T19:05:31.108888Z","iopub.status.idle":"2025-04-03T19:05:31.141162Z","shell.execute_reply.started":"2025-04-03T19:05:31.108864Z","shell.execute_reply":"2025-04-03T19:05:31.139370Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-d7c6c648ec15>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 3 - Calcolo MAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmae_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean_abslute_error\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmae_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean_abslute_error\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_valadation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmae_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean_abslute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'mean_abslute_error' is not defined"],"ename":"NameError","evalue":"name 'mean_abslute_error' is not defined","output_type":"error"}],"execution_count":32},{"cell_type":"markdown","source":"### **Esercizio: Crea una funzione che esegua una pipeline di Regressione Lineare**\n\nLa funzione deve richiedere un parametro `hyperparams` per gestire i diversi casi. \n\n`hyperparams` deve essere un dizionario contenente diverse chiavi, in base al valore di queste chiavi devono essere eseguiti (oppure no) diversi pezzi di codice. \n\nIn questo esercizio la chiave da utilizzare sarà `hyperparams['data_standardize']`. Se il valore di questa chiave sarà **True** allora eseguire la standardizzazione con `scikit-learn`, se invece è **False** non verrà eseguita alcuna standardizzazione.\n\n**Step 1:** Controllare se eseguire o no la standardizzazione.\n\n* **Step 1.1:** Scrivere il codice per eseguire la standardizzazione.\n\n**Step 2:** Utilizza `np.c_` per concatenare una colonna di uno alle matrici delle caratteristiche.\n\n**Step 3:** Applichiamo la formula matematica della regressione lineare.\n\n**Step 4:** Calcolo MAE utilizzando la formula (NON con `scikit-learn`).\n\nLa funzione deve ritornare i valori della MAE.","metadata":{}},{"cell_type":"markdown","source":"Dopo aver testato i risultati con `hyperparams['data_standardize']` = **True**, provare anche i risultati ottenuti se `hyperparams['data_standardize']` = **False**.","metadata":{}},{"cell_type":"code","source":"# svolgimento...\n\ndef pipeline(X_train, y_train, X_val, y_val, hyperparams):\n\n    X_train = np.array(X_train, dtype=float)\n    y_train = np.array(y_train, dtype=float)\n    X_val = np.array(X_val, dtype=float)\n    y_val = np.array(y_val, dtype=float)\n    \n    # Step 1 - Controllo se è richiesta la standardizzazione dei dati\n      if hyperparams['data_standardize']:  \n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_val = scaler.transform(X_val)\n\n    # Step 2 - Concatenare una colonna di uno alla matrice delle features (per il termine di bias)\n    X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n    X_val = np.c_[np.ones(X_val.shape[0]), X_val]\n\n    # Step 3 - Applicare la formula della regressione lineare e calcolare predizioni\n    theta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train\n\n    # Predizioni sui dati di validazione\n    y_pred = X_val @ theta\n\n    # Step 4 - Calcolo MAE manualmente\n    mae = np.mean(np.abs(y_pred - y_val))\n\n    return mae\n","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:31.141716Z","iopub.status.idle":"2025-04-03T19:05:31.142177Z","shell.execute_reply":"2025-04-03T19:05:31.142005Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hyperparams = {'data_standardize': True}\n\ntrain_fraction = 0.8\nvalidation_fraction = 0.2\n\nnum_train = int(train_fraction * X.shape[0])\n\nX_train = X[:num_train]\ny_train = y[:num_train]\n\nX_validation = X[num_train:]\ny_validation = y[num_train:]\n\n# Chiamare la funzione pipeline e stampare i risultati della MAE\nmae_standardized = pipeline(X_train, y_train, X_validation, y_validation, hyperparams)\nprint(\"MAE con standardizzazione:\", mae_standardized)\nmae_standardized = pipeline(X_train, y_train, X_validation, y_validation, hyperparams)\nprint(\"MAE con standardizzazione:\", mae_standardized)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:31.143094Z","iopub.status.idle":"2025-04-03T19:05:31.143598Z","shell.execute_reply":"2025-04-03T19:05:31.143445Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Esercizio: Implementare alla funzione `pipeline` la possibilità di usare PCA**\n\nModifichiamo la funzione `pipeline` in modo da gestire anche la possibilità di effettuare la PCA. Dunque aggiungiamo al dizionario `hyperparams` la chiave `use_pca`. \n\nSe `hyperparams['use_pca']` = **True** verrà eseguita la PCA. \n\nSe `hyperparams['use_pca']` = **False** non verrà eseguita la PCA.\n\nLa gestione della standardizzazione deve essere mantenuta come prima.","metadata":{}},{"cell_type":"code","source":"# svolgimento...\nfrom sklearn.decomposition import PCA\n\ndef pipeline(X_train, y_train, X_val, y_val, hyperparams):\n\n    X_train = np.array(X_train, dtype=float)\n    y_train = np.array(y_train, dtype=float)\n    X_val = np.array(X_val, dtype=float)\n    y_val = np.array(y_val, dtype=float)\n\n    # Step 1 - Controllo se è richista la PCA\n    if hyperparams['use_pca']:\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_val = scaler.transform(X_val)  \n        # Step 1.1 - Scrivere il codice per applicare PCA\n    \n    # Step 2 - Controllo se è richiesta la standardizzazione dei dati\n    if hyperparams['data_standardize']:\n        n_components = hyperparams.get('pca_components', min(X_train.shape[1], X_train.shape[0]))  # Numero di componenti\n        pca = PCA(n_components=n_components)\n        X_train = pca.fit_transform(X_train)\n        X_val = pca.transform(X_val)\n        # Step 2.1 - Scrivere il codice per standardizzare i dati \n\n\n    # Step 3 - Concatenare una colonna di uno alla matrice delle features\n    X_train = np.c_[np.ones((X_train.shape[0], 1)), X_train]  \n    X_val = np.c_[np.ones((X_val.shape[0], 1)), X_val] \n    # Step 4 - Applicare formula della regressione lineare e calcolare predizioni\n    theta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train \n    # Step 5 - Calcolare MAE \n    mae = np.mean(np.abs(y_pred - y_val))  \n\n    return mae","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:31.144317Z","iopub.status.idle":"2025-04-03T19:05:31.144777Z","shell.execute_reply":"2025-04-03T19:05:31.144594Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hyperparams = {'data_standardize': True, 'use_pca': True}\ntrain_fraction = 0.8\nvalidation_fraction = 0.2\n\nnum_train = int(train_fraction * X.shape[0])\n\nX_train = X[:num_train]\ny_train = y[:num_train]\n\nX_validation = X[num_train:]\ny_validation = y[num_train:]\n\n# Chiamare la funzione pipeline e stampare i risultati della MAE al variare dell' utilizzo della PCA.\nhyperparams_pca = {'data_standardize': True, 'use_pca': True, 'pca_components': 2}\nmae_pca = pipeline(X_train, y_train, X_validation, y_validation, hyperparams_pca)\nprint(\"MAE con PCA:\", mae_pca)\n\nhyperparams_no_pca = {'data_standardize': True, 'use_pca': False}\nmae_no_pca = pipeline(X_train, y_train, X_validation, y_validation, hyperparams_no_pca)\nprint(\"MAE senza PCA:\", mae_no_pca)","metadata":{"execution":{"iopub.status.busy":"2025-04-03T19:05:31.145525Z","iopub.status.idle":"2025-04-03T19:05:31.146079Z","shell.execute_reply":"2025-04-03T19:05:31.145839Z"},"trusted":true},"outputs":[],"execution_count":null}]}